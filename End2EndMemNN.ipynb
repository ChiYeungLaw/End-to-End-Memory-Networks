{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jIxxTGqpNpyT"
   },
   "source": [
    "# End to End Memory Network Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UytNRQqN-1F"
   },
   "source": [
    "In this experiment, I will base on the paper \"End-to-end memory networks\" to implement an `MemN2N` and test this network with a QA task, `bAbI v1.1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4l2eWdoCOfrS"
   },
   "source": [
    "## Experiment Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKT_2IRlPVUP"
   },
   "source": [
    "- Colab\n",
    "- Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0qPPW_qPftV"
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tk5K6ptQPrse"
   },
   "source": [
    "Let's firs see the [bAbl](https://research.fb.com/downloads/babi/) dataset. The `bAbl` dataset contains 20 tasks for testing text understanding and reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tNtShbWuTfoo"
   },
   "source": [
    "The file format of each task is as follows:\n",
    "```\n",
    "ID text\n",
    "ID text\n",
    "ID text\n",
    "ID question \\t answer \\t supporting fact IDs\n",
    "```\n",
    "For example:\n",
    "```\n",
    "1 Mary moved to the bathroom.\n",
    "2 John went to the hallway.\n",
    "3 Where is Mary?        bathroom        1\n",
    "4 Daniel went back to the hallway.\n",
    "5 Sandra moved to the garden.\n",
    "6 Where is Daniel?      hallway 4\n",
    "7 John moved to the office.\n",
    "8 Sandra journeyed to the bathroom.\n",
    "9 Where is Daniel?      hallway 4\n",
    "10 Mary moved to the hallway.\n",
    "11 Daniel travelled to the office.\n",
    "12 Where is Daniel?     office  11\n",
    "13 John went back to the garden.\n",
    "14 John moved to the bedroom.\n",
    "15 Where is Sandra?     bathroom        8\n",
    "```\n",
    "We can see that a given QA task contains a set of statements, followed by a question whose answer is typically a single word. Our job here is to let our network read the story and learn to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first check our files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = \"/home/chiyeung/Datasets/bAbI/Tasks20/tasksv11/en\"\n",
    "data_dir = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qa10_indefinite-knowledge_test.txt',\n",
       " 'qa10_indefinite-knowledge_train.txt',\n",
       " 'qa11_basic-coreference_test.txt',\n",
       " 'qa11_basic-coreference_train.txt',\n",
       " 'qa12_conjunction_test.txt',\n",
       " 'qa12_conjunction_train.txt',\n",
       " 'qa13_compound-coreference_test.txt',\n",
       " 'qa13_compound-coreference_train.txt',\n",
       " 'qa14_time-reasoning_test.txt',\n",
       " 'qa14_time-reasoning_train.txt',\n",
       " 'qa15_basic-deduction_test.txt',\n",
       " 'qa15_basic-deduction_train.txt',\n",
       " 'qa16_basic-induction_test.txt',\n",
       " 'qa16_basic-induction_train.txt',\n",
       " 'qa17_positional-reasoning_test.txt',\n",
       " 'qa17_positional-reasoning_train.txt',\n",
       " 'qa18_size-reasoning_test.txt',\n",
       " 'qa18_size-reasoning_train.txt',\n",
       " 'qa19_path-finding_test.txt',\n",
       " 'qa19_path-finding_train.txt',\n",
       " 'qa1_single-supporting-fact_test.txt',\n",
       " 'qa1_single-supporting-fact_train.txt',\n",
       " 'qa20_agents-motivations_test.txt',\n",
       " 'qa20_agents-motivations_train.txt',\n",
       " 'qa2_two-supporting-facts_test.txt',\n",
       " 'qa2_two-supporting-facts_train.txt',\n",
       " 'qa3_three-supporting-facts_test.txt',\n",
       " 'qa3_three-supporting-facts_train.txt',\n",
       " 'qa4_two-arg-relations_test.txt',\n",
       " 'qa4_two-arg-relations_train.txt',\n",
       " 'qa5_three-arg-relations_test.txt',\n",
       " 'qa5_three-arg-relations_train.txt',\n",
       " 'qa6_yes-no-questions_test.txt',\n",
       " 'qa6_yes-no-questions_train.txt',\n",
       " 'qa7_counting_test.txt',\n",
       " 'qa7_counting_train.txt',\n",
       " 'qa8_lists-sets_test.txt',\n",
       " 'qa8_lists-sets_train.txt',\n",
       " 'qa9_simple-negation_test.txt',\n",
       " 'qa9_simple-negation_train.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: qa10_indefinite-knowledge_train.txt.\n",
      "\n",
      "1 Fred is either in the school or the park.\n",
      "\n",
      "2 Mary went back to the office.\n",
      "\n",
      "3 Is Mary in the office? \tyes\t2\n",
      "\n",
      "4 Bill is either in the kitchen or the park.\n",
      "\n",
      "5 Fred moved to the cinema.\n",
      "\n",
      "6 Is Fred in the park? \tno\t5\n",
      "\n",
      "7 Fred is in the office.\n",
      "\n",
      "8 Bill moved to the cinema.\n",
      "\n",
      "9 Is Bill in the cinema? \tyes\t8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_1_path = os.path.join(data_path, data_dir[1])\n",
    "print(f\"File: {data_dir[1]}.\\n\")\n",
    "with open(data_1_path, encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines[:9]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that every question is just based on the sentences above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need a method to tokenize a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenize(sent):\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need a method to read a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tasks(data_dir, task_id):\n",
    "    assert task_id > 0 and task_id < 21\n",
    "    files = os.listdir(data_dir)\n",
    "    files = [os.path.join(data_dir, f) for f in files]\n",
    "    s = 'qa{}_'.format(task_id)\n",
    "    \n",
    "    # read the task_id train and test file\n",
    "    train_files = [f for f in files if s in f and 'train' in f][0]\n",
    "    test_files = [f for f in files if s in f and 'test' in f][0]\n",
    "    \n",
    "    # read train data and test data from two files\n",
    "    train_data = get_stories(train_files)\n",
    "    test_data = get_stories(test_files)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a method to get the stories, queries and answers from the task file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stories(task_file):\n",
    "    with open(task_file, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return parse_story(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_story(lines):\n",
    "    data = []\n",
    "    story = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.lower()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            # This line is a question\n",
    "            q, a, _ = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            a = [a]\n",
    "            substory = None\n",
    "            \n",
    "            # remove question marks\n",
    "            if q[-1] == '?':\n",
    "                q = q[:-1]\n",
    "            \n",
    "            substory = [x for x in story if x]\n",
    "            \n",
    "            data.append((substory[::-1], q, a)) # reverse story\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            if sent[-1] == '.':\n",
    "                sent = sent[:-1]\n",
    "            story.append(sent)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a method to load all the tasks together to form a big training data set or read the task one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def load_data(data_dir, joint_training, task_num):\n",
    "    if joint_training == 0:\n",
    "        start_task = task_num\n",
    "        end_task = task_num\n",
    "    else:\n",
    "        start_task = 1\n",
    "        end_task = 20\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    while start_task <= end_task:\n",
    "        train_task, test_task = load_tasks(data_dir, task_num)\n",
    "        train_data += train_task\n",
    "        test_data += test_task\n",
    "        start_task += 1\n",
    "        \n",
    "    data = train_data + test_data\n",
    "    vocab = []\n",
    "    for s, q, a in data:\n",
    "        vocab += list(chain.from_iterable(s))\n",
    "        vocab += q\n",
    "        vocab += a\n",
    "    vocab = list(set(vocab))\n",
    "    return train_data, test_data, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a method to transform a word sentence to a index sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_index(sent, w2i):\n",
    "    \"\"\"\n",
    "    sent: a word sentence\n",
    "    w2i: a mapping, word -> index\n",
    "    \"\"\"\n",
    "    vec = []\n",
    "    for w in sent:\n",
    "        if w in w2i:\n",
    "            vec.append(w2i[w])\n",
    "        else:\n",
    "            vec.append(w2i['<PAD>'])\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function to vectorize all the data, pad all the stories to the same length, pad all the storyies' sentences to the same length and pad all the queries to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data, w2i, story_len, sent_len, q_len):\n",
    "    vec_data = []\n",
    "    for d in data:\n",
    "        tmp_story = d[0]\n",
    "        story = []\n",
    "        for s in tmp_story:\n",
    "            sent = word_to_index(s, w2i)\n",
    "            if len(sent) < sent_len:\n",
    "                sent += [0] * (sent_len - len(sent))\n",
    "            story.append(sent)\n",
    "        while len(story) < story_len:\n",
    "            story.append([0] * sent_len)\n",
    "        story = story[:story_len]\n",
    "        q = d[1]\n",
    "        q = word_to_index(q, w2i)\n",
    "        if len(q) < q_len:\n",
    "            q += [0] * (q_len - len(q))\n",
    "        a = d[2]\n",
    "        a = word_to_index(a, w2i)\n",
    "        vec_data.append((story, q, a))\n",
    "    return vec_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Arcitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will build the class of MemN2N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MemN2N(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, story_len, device, hops=3, dropout_rate=0.2):\n",
    "        super(MemN2N, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.hops = hops\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.A = nn.ModuleList([nn.Embedding(vocab_size, embed_size) for _ in range(hops+1)])\n",
    "        for i in range(len(self.A)):\n",
    "            self.A[i].weight.data.normal_(0, 0.1)\n",
    "            self.A[i].weight.data[0] = 0\n",
    "        self.B = self.A[0]\n",
    "        self.TA = nn.Parameter(torch.Tensor(1, story_len + 1, embed_size).normal_(0, 0.1))\n",
    "        self.TC = nn.Parameter(torch.Tensor(1, story_len + 1, embed_size).normal_(0, 0.1))\n",
    "        self.proj = nn.Linear(embed_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, q):\n",
    "        \"\"\"\n",
    "        x: input story, [batch_size, story_len, sent_len]\n",
    "        q: query, [batch_size, q_len]\n",
    "        \"\"\"\n",
    "        batch_size, story_len, sent_len = x.shape\n",
    "\n",
    "        # positional encoding\n",
    "        J = sent_len\n",
    "        d = self.embed_size\n",
    "        position = torch.arange(0, J).reshape(-1, 1)\n",
    "        pe = (1 - position / J) - (torch.arange(1, d+1) / d) * (1 - 2 * position / J)\n",
    "        pe = pe.to(device, torch.float) # [sent_len, embed_size]\n",
    "        pe = pe.unsqueeze(0).unsqueeze(0) # [1, 1, sent_len, embed_size]\n",
    "        pe = pe.repeat(batch_size, story_len, 1, 1) # [batch_size, story_len, sent_len, embed_size]\n",
    "        \n",
    "        # query embedding\n",
    "        u = self.dropout(self.B(q)) # [batch_size, q_len, embed_size]\n",
    "        u = u.sum(dim=1) # [batch_size, embed_size]\n",
    "        \n",
    "        for k in range(self.hops):\n",
    "            #---- m ----\n",
    "            m = self.dropout(self.A[k](x)) # [batch_size, story_len, sent_len, embed_size]\n",
    "            m = m * pe # [batch_size, story_len, sent_len, embed_size]\n",
    "            m = m.sum(dim=2) # [batch_size, story_len, embed_size]\n",
    "            m += self.TA.repeat(batch_size, 1, 1)[:, :story_len, :] # [batch_size, story_len, embed_size]\n",
    "            \n",
    "            #---- p ----\n",
    "            u = u.unsqueeze(1) # [batch_size, 1, embed_size]\n",
    "            p = u * m # [batch_size, story_len, embed_size]\n",
    "            p = p.sum(dim=2) # [batch_size, story_len]\n",
    "            p = torch.softmax(p, dim=-1)\n",
    "            u = u.squeeze(1) # [batch_size, embed_size]\n",
    "            \n",
    "            #---- c ----\n",
    "            c = self.dropout(self.A[k+1](x)) # [batch_size, story_len, sent_len, embed_size]\n",
    "            c *= pe # [batch_size, story_len, sent_len, embed_size]\n",
    "            c = c.sum(dim=2) # [batch_size, story_len, embed_size]\n",
    "            c += self.TC.repeat(batch_size, 1, 1)[:, :story_len, :] # [batch_size, story_len. embed_size]\n",
    "            \n",
    "            #---- o ----\n",
    "            p = p.unsqueeze(2) # [batch_size, story_len, 1]\n",
    "            o = p * c # [batch_size, story_len, embed_size]\n",
    "            o = o.sum(dim=1) # [batch_size, embed_size]\n",
    "            \n",
    "            #---- new u ----\n",
    "            u = u + o # [batch_size, embed_size]\n",
    "        \n",
    "        # Linear projection\n",
    "        out = self.proj(u) # [batch_size, vocab_size]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Traning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def train(model, train_data, batch_size, num_epochs, optimizer,\n",
    "          criterion, w2i, max_story_len, device):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.\n",
    "        random.shuffle(train_data)\n",
    "        for i in range(0, len(train_data)-batch_size, batch_size):\n",
    "            batch_data = train_data[i:i+batch_size]\n",
    "            story = [d[0] for d in batch_data]\n",
    "            # compute the number of story\n",
    "            story_len = min(max_story_len, max([len(s) for s in story]))\n",
    "            # compute the length of each sentence in a story\n",
    "            s_sent_len = max([len(sent) for s in story for sent in s])\n",
    "        \n",
    "            # compute query sentence length\n",
    "            query = [d[1] for d in batch_data]\n",
    "            q_sent_len = max([len(q) for q in query])\n",
    "\n",
    "            # Then we need to vectorize all the data\n",
    "            vec_data = vectorize(batch_data, w2i, story_len, s_sent_len, q_sent_len)\n",
    "            story = torch.LongTensor([d[0] for d in vec_data]).to(device)\n",
    "            query = torch.LongTensor([d[1] for d in vec_data]).to(device)\n",
    "            a = torch.LongTensor([d[2][0] for d in vec_data]).to(device)\n",
    "        \n",
    "            # model prediction\n",
    "            pred = model(story, query)\n",
    "        \n",
    "            # loss computation\n",
    "            loss = criterion(pred, a)\n",
    "        \n",
    "            # optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            # reset padding index weight\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if 'A.' in name:\n",
    "                        param.data[0] = 0\n",
    "        \n",
    "            # gradient clipping\n",
    "            for p in model.parameters():\n",
    "                torch.nn.utils.clip_grad_norm_(p, 40.0)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        if (epoch+1) % 20 == 0:\n",
    "            print(f\"Epoch:{epoch+1}. Loss:{total_loss/batch_size:.2f}.\")\n",
    "            total_loss = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data, batch_size, w2i, max_story_len, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(0, len(test_data)-batch_size, batch_size):\n",
    "        batch_data = test_data[i:i+batch_size]\n",
    "        \n",
    "        # compute the number of story\n",
    "        story = [d[0] for d in batch_data]\n",
    "        story_len = min(max_story_len, max([len(s) for s in story]))\n",
    "        \n",
    "        # compute the length of a sentence of a story\n",
    "        s_sent_len = max([len(sent) for s in story for sent in s])\n",
    "        \n",
    "        # comptue the length of a query\n",
    "        query = [d[1] for d in batch_data]\n",
    "        q_sent_len = max([len(q) for q in query])\n",
    "        \n",
    "        # Then we vectorize all the data\n",
    "        vec_data = vectorize(batch_data, w2i, story_len, s_sent_len, q_sent_len)\n",
    "        story = torch.LongTensor([d[0] for d in vec_data]).to(device)\n",
    "        query = torch.LongTensor([d[1] for d in vec_data]).to(device)\n",
    "        a = torch.LongTensor([d[2][0] for d in vec_data]).to(device)\n",
    "        \n",
    "        # prediction\n",
    "        pred = model(story, query)\n",
    "        _, pred = pred.max(dim=-1)\n",
    "        correct += (pred==a).sum().item()\n",
    "        total += len(batch_data)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Running function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# hyparameter\n",
    "num_epochs = 200\n",
    "max_story_len = 25\n",
    "embed_size = 30\n",
    "batch_size = 32\n",
    "\n",
    "# criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def run():\n",
    "    test_acc_results = []\n",
    "    for task_id in range(1, 21):\n",
    "        print(f\"==== Task {task_id} ====\")\n",
    "        train_data, test_data, vocab = load_data(data_path, 0, task_id)\n",
    "        w2i = dict((w, i) for i, w in enumerate(vocab, 1))\n",
    "        w2i['<PAD>'] = 0\n",
    "        vocab_size = len(w2i)\n",
    "        story_len = min(max_story_len, max([len(s) for s, q, a in train_data + test_data]))\n",
    "        model = MemN2N(vocab_size, embed_size, story_len, device).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "        train(model, train_data, batch_size, num_epochs, optimizer,\n",
    "              criterion, w2i, story_len, device)\n",
    "        acc = test(model, test_data, batch_size, w2i, story_len, device)\n",
    "        print(f\"Task {task_id} test acc: {acc:.2%}.\")\n",
    "        test_acc_results.append(acc)\n",
    "    return test_acc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Task 1 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiyeung/anaconda3/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20. Loss:0.22.\n",
      "Epoch:40. Loss:0.12.\n",
      "Epoch:60. Loss:0.08.\n",
      "Epoch:80. Loss:0.07.\n",
      "Epoch:100. Loss:0.03.\n",
      "Epoch:120. Loss:0.04.\n",
      "Epoch:140. Loss:0.02.\n",
      "Epoch:160. Loss:0.02.\n",
      "Epoch:180. Loss:0.02.\n",
      "Epoch:200. Loss:0.02.\n",
      "Task 1 test acc: 100.00%.\n",
      "==== Task 2 ====\n",
      "Epoch:20. Loss:1.44.\n",
      "Epoch:40. Loss:1.26.\n",
      "Epoch:60. Loss:1.11.\n",
      "Epoch:80. Loss:0.95.\n",
      "Epoch:100. Loss:0.77.\n",
      "Epoch:120. Loss:0.61.\n",
      "Epoch:140. Loss:0.50.\n",
      "Epoch:160. Loss:0.46.\n",
      "Epoch:180. Loss:0.36.\n",
      "Epoch:200. Loss:0.29.\n",
      "Task 2 test acc: 65.83%.\n",
      "==== Task 3 ====\n",
      "Epoch:20. Loss:1.42.\n",
      "Epoch:40. Loss:1.12.\n",
      "Epoch:60. Loss:0.99.\n",
      "Epoch:80. Loss:0.83.\n",
      "Epoch:100. Loss:0.78.\n",
      "Epoch:120. Loss:0.69.\n",
      "Epoch:140. Loss:0.73.\n",
      "Epoch:160. Loss:0.63.\n",
      "Epoch:180. Loss:0.60.\n",
      "Epoch:200. Loss:0.54.\n",
      "Task 3 test acc: 42.24%.\n",
      "==== Task 4 ====\n",
      "Epoch:20. Loss:0.90.\n",
      "Epoch:40. Loss:0.64.\n",
      "Epoch:60. Loss:0.60.\n",
      "Epoch:80. Loss:0.55.\n",
      "Epoch:100. Loss:0.52.\n",
      "Epoch:120. Loss:0.52.\n",
      "Epoch:140. Loss:0.53.\n",
      "Epoch:160. Loss:0.51.\n",
      "Epoch:180. Loss:0.50.\n",
      "Epoch:200. Loss:0.49.\n",
      "Task 4 test acc: 75.20%.\n",
      "==== Task 5 ====\n",
      "Epoch:20. Loss:0.48.\n",
      "Epoch:40. Loss:0.31.\n",
      "Epoch:60. Loss:0.24.\n",
      "Epoch:80. Loss:0.20.\n",
      "Epoch:100. Loss:0.17.\n",
      "Epoch:120. Loss:0.16.\n",
      "Epoch:140. Loss:0.16.\n",
      "Epoch:160. Loss:0.15.\n",
      "Epoch:180. Loss:0.12.\n",
      "Epoch:200. Loss:0.12.\n",
      "Task 5 test acc: 81.25%.\n",
      "==== Task 6 ====\n",
      "Epoch:20. Loss:0.63.\n",
      "Epoch:40. Loss:0.43.\n",
      "Epoch:60. Loss:0.25.\n",
      "Epoch:80. Loss:0.20.\n",
      "Epoch:100. Loss:0.16.\n",
      "Epoch:120. Loss:0.12.\n",
      "Epoch:140. Loss:0.12.\n",
      "Epoch:160. Loss:0.11.\n",
      "Epoch:180. Loss:0.13.\n",
      "Epoch:200. Loss:0.10.\n",
      "Task 6 test acc: 84.38%.\n",
      "==== Task 7 ====\n",
      "Epoch:20. Loss:0.45.\n",
      "Epoch:40. Loss:0.39.\n",
      "Epoch:60. Loss:0.32.\n",
      "Epoch:80. Loss:0.29.\n",
      "Epoch:100. Loss:0.25.\n",
      "Epoch:120. Loss:0.24.\n",
      "Epoch:140. Loss:0.24.\n",
      "Epoch:160. Loss:0.21.\n",
      "Epoch:180. Loss:0.20.\n",
      "Epoch:200. Loss:0.20.\n",
      "Task 7 test acc: 79.23%.\n",
      "==== Task 8 ====\n",
      "Epoch:20. Loss:0.58.\n",
      "Epoch:40. Loss:0.43.\n",
      "Epoch:60. Loss:0.32.\n",
      "Epoch:80. Loss:0.28.\n",
      "Epoch:100. Loss:0.21.\n",
      "Epoch:120. Loss:0.24.\n",
      "Epoch:140. Loss:0.20.\n",
      "Epoch:160. Loss:0.17.\n",
      "Epoch:180. Loss:0.18.\n",
      "Epoch:200. Loss:0.15.\n",
      "Task 8 test acc: 87.30%.\n",
      "==== Task 9 ====\n",
      "Epoch:20. Loss:0.57.\n",
      "Epoch:40. Loss:0.44.\n",
      "Epoch:60. Loss:0.37.\n",
      "Epoch:80. Loss:0.29.\n",
      "Epoch:100. Loss:0.26.\n",
      "Epoch:120. Loss:0.23.\n",
      "Epoch:140. Loss:0.20.\n",
      "Epoch:160. Loss:0.18.\n",
      "Epoch:180. Loss:0.17.\n",
      "Epoch:200. Loss:0.19.\n",
      "Task 9 test acc: 85.18%.\n",
      "==== Task 10 ====\n",
      "Epoch:20. Loss:0.77.\n",
      "Epoch:40. Loss:0.62.\n",
      "Epoch:60. Loss:0.51.\n",
      "Epoch:80. Loss:0.42.\n",
      "Epoch:100. Loss:0.35.\n",
      "Epoch:120. Loss:0.28.\n",
      "Epoch:140. Loss:0.29.\n",
      "Epoch:160. Loss:0.25.\n",
      "Epoch:180. Loss:0.26.\n",
      "Epoch:200. Loss:0.23.\n",
      "Task 10 test acc: 82.66%.\n",
      "==== Task 11 ====\n",
      "Epoch:20. Loss:0.40.\n",
      "Epoch:40. Loss:0.30.\n",
      "Epoch:60. Loss:0.27.\n",
      "Epoch:80. Loss:0.23.\n",
      "Epoch:100. Loss:0.23.\n",
      "Epoch:120. Loss:0.20.\n",
      "Epoch:140. Loss:0.17.\n",
      "Epoch:160. Loss:0.17.\n",
      "Epoch:180. Loss:0.13.\n",
      "Epoch:200. Loss:0.16.\n",
      "Task 11 test acc: 88.41%.\n",
      "==== Task 12 ====\n",
      "Epoch:20. Loss:0.31.\n",
      "Epoch:40. Loss:0.13.\n",
      "Epoch:60. Loss:0.10.\n",
      "Epoch:80. Loss:0.07.\n",
      "Epoch:100. Loss:0.05.\n",
      "Epoch:120. Loss:0.05.\n",
      "Epoch:140. Loss:0.07.\n",
      "Epoch:160. Loss:0.06.\n",
      "Epoch:180. Loss:0.05.\n",
      "Epoch:200. Loss:0.05.\n",
      "Task 12 test acc: 100.00%.\n",
      "==== Task 13 ====\n",
      "Epoch:20. Loss:0.28.\n",
      "Epoch:40. Loss:0.21.\n",
      "Epoch:60. Loss:0.19.\n",
      "Epoch:80. Loss:0.16.\n",
      "Epoch:100. Loss:0.15.\n",
      "Epoch:120. Loss:0.13.\n",
      "Epoch:140. Loss:0.13.\n",
      "Epoch:160. Loss:0.12.\n",
      "Epoch:180. Loss:0.14.\n",
      "Epoch:200. Loss:0.12.\n",
      "Task 13 test acc: 92.14%.\n",
      "==== Task 14 ====\n",
      "Epoch:20. Loss:0.91.\n",
      "Epoch:40. Loss:0.42.\n",
      "Epoch:60. Loss:0.30.\n",
      "Epoch:80. Loss:0.27.\n",
      "Epoch:100. Loss:0.19.\n",
      "Epoch:120. Loss:0.19.\n",
      "Epoch:140. Loss:0.15.\n",
      "Epoch:160. Loss:0.10.\n",
      "Epoch:180. Loss:0.12.\n",
      "Epoch:200. Loss:0.07.\n",
      "Task 14 test acc: 96.98%.\n",
      "==== Task 15 ====\n",
      "Epoch:20. Loss:1.06.\n",
      "Epoch:40. Loss:0.96.\n",
      "Epoch:60. Loss:0.94.\n",
      "Epoch:80. Loss:0.86.\n",
      "Epoch:100. Loss:0.75.\n",
      "Epoch:120. Loss:0.56.\n",
      "Epoch:140. Loss:0.37.\n",
      "Epoch:160. Loss:0.26.\n",
      "Epoch:180. Loss:0.22.\n",
      "Epoch:200. Loss:0.19.\n",
      "Task 15 test acc: 96.47%.\n",
      "==== Task 16 ====\n",
      "Epoch:20. Loss:1.04.\n",
      "Epoch:40. Loss:0.95.\n",
      "Epoch:60. Loss:0.93.\n",
      "Epoch:80. Loss:0.86.\n",
      "Epoch:100. Loss:0.86.\n",
      "Epoch:120. Loss:0.85.\n",
      "Epoch:140. Loss:0.83.\n",
      "Epoch:160. Loss:0.79.\n",
      "Epoch:180. Loss:0.78.\n",
      "Epoch:200. Loss:0.77.\n",
      "Task 16 test acc: 49.60%.\n",
      "==== Task 17 ====\n",
      "Epoch:20. Loss:0.67.\n",
      "Epoch:40. Loss:0.66.\n",
      "Epoch:60. Loss:0.66.\n",
      "Epoch:80. Loss:0.64.\n",
      "Epoch:100. Loss:0.63.\n",
      "Epoch:120. Loss:0.62.\n",
      "Epoch:140. Loss:0.62.\n",
      "Epoch:160. Loss:0.60.\n",
      "Epoch:180. Loss:0.60.\n",
      "Epoch:200. Loss:0.59.\n",
      "Task 17 test acc: 54.23%.\n",
      "==== Task 18 ====\n",
      "Epoch:20. Loss:0.64.\n",
      "Epoch:40. Loss:0.61.\n",
      "Epoch:60. Loss:0.60.\n",
      "Epoch:80. Loss:0.57.\n",
      "Epoch:100. Loss:0.55.\n",
      "Epoch:120. Loss:0.55.\n",
      "Epoch:140. Loss:0.53.\n",
      "Epoch:160. Loss:0.52.\n",
      "Epoch:180. Loss:0.50.\n",
      "Epoch:200. Loss:0.50.\n",
      "Task 18 test acc: 53.93%.\n",
      "==== Task 19 ====\n",
      "Epoch:20. Loss:2.37.\n",
      "Epoch:40. Loss:2.30.\n",
      "Epoch:60. Loss:2.25.\n",
      "Epoch:80. Loss:2.22.\n",
      "Epoch:100. Loss:2.18.\n",
      "Epoch:120. Loss:2.17.\n",
      "Epoch:140. Loss:2.14.\n",
      "Epoch:160. Loss:2.14.\n",
      "Epoch:180. Loss:2.11.\n",
      "Epoch:200. Loss:2.10.\n",
      "Task 19 test acc: 11.90%.\n",
      "==== Task 20 ====\n",
      "Epoch:20. Loss:0.14.\n",
      "Epoch:40. Loss:0.13.\n",
      "Epoch:60. Loss:0.12.\n",
      "Epoch:80. Loss:0.09.\n",
      "Epoch:100. Loss:0.06.\n",
      "Epoch:120. Loss:0.02.\n",
      "Epoch:140. Loss:0.01.\n",
      "Epoch:160. Loss:0.01.\n",
      "Epoch:180. Loss:0.00.\n",
      "Epoch:200. Loss:0.00.\n",
      "Task 20 test acc: 99.70%.\n"
     ]
    }
   ],
   "source": [
    "test_acc_res = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>task 1</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 2</th>\n",
       "      <td>0.658266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 3</th>\n",
       "      <td>0.422379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 4</th>\n",
       "      <td>0.752016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 5</th>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 6</th>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 7</th>\n",
       "      <td>0.792339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 8</th>\n",
       "      <td>0.872984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 9</th>\n",
       "      <td>0.851815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 10</th>\n",
       "      <td>0.826613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 11</th>\n",
       "      <td>0.884073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 12</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 13</th>\n",
       "      <td>0.921371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 14</th>\n",
       "      <td>0.969758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 15</th>\n",
       "      <td>0.964718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 16</th>\n",
       "      <td>0.495968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 17</th>\n",
       "      <td>0.542339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 18</th>\n",
       "      <td>0.539315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 19</th>\n",
       "      <td>0.118952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task 20</th>\n",
       "      <td>0.996976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              acc\n",
       "task 1   1.000000\n",
       "task 2   0.658266\n",
       "task 3   0.422379\n",
       "task 4   0.752016\n",
       "task 5   0.812500\n",
       "task 6   0.843750\n",
       "task 7   0.792339\n",
       "task 8   0.872984\n",
       "task 9   0.851815\n",
       "task 10  0.826613\n",
       "task 11  0.884073\n",
       "task 12  1.000000\n",
       "task 13  0.921371\n",
       "task 14  0.969758\n",
       "task 15  0.964718\n",
       "task 16  0.495968\n",
       "task 17  0.542339\n",
       "task 18  0.539315\n",
       "task 19  0.118952\n",
       "task 20  0.996976"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "task = []\n",
    "\n",
    "for i in range(1, 21):\n",
    "    task_id = \"task {}\".format(i)\n",
    "    task.append(task_id)\n",
    "\n",
    "df = pd.DataFrame(test_acc_res, index=task, columns=['acc'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "End2EndMemNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
